{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd99b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers==4.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a7bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "067c2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important libraries\n",
    "import os\n",
    "import datasets\n",
    "from transformers import VisionEncoderDecoderModel, AutoFeatureExtractor,AutoTokenizer\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a681cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b73545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.11.crossattention.q_attn.weight', 'h.2.ln_cross_attn.weight', 'h.2.crossattention.q_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.8.ln_cross_attn.weight', 'h.5.crossattention.c_proj.weight', 'h.8.crossattention.c_proj.weight', 'h.10.crossattention.c_attn.weight', 'h.11.crossattention.c_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.9.ln_cross_attn.weight', 'h.8.crossattention.bias', 'h.11.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.weight', 'h.3.crossattention.bias', 'h.7.crossattention.c_proj.bias', 'h.9.crossattention.q_attn.weight', 'h.1.crossattention.c_proj.weight', 'h.8.crossattention.masked_bias', 'h.2.crossattention.bias', 'h.3.crossattention.q_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.10.ln_cross_attn.weight', 'h.10.crossattention.masked_bias', 'h.4.crossattention.bias', 'h.4.crossattention.masked_bias', 'h.7.crossattention.q_attn.weight', 'h.6.ln_cross_attn.weight', 'h.1.ln_cross_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.0.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.6.crossattention.masked_bias', 'h.4.crossattention.c_proj.weight', 'h.6.crossattention.bias', 'h.1.crossattention.q_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.3.ln_cross_attn.weight', 'h.8.crossattention.c_attn.weight', 'h.0.ln_cross_attn.weight', 'h.11.crossattention.masked_bias', 'h.7.crossattention.masked_bias', 'h.11.crossattention.bias', 'h.4.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.7.crossattention.bias', 'h.9.crossattention.c_proj.bias', 'h.7.crossattention.c_attn.weight', 'h.6.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.9.crossattention.c_attn.weight', 'h.1.crossattention.bias', 'h.4.crossattention.q_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.bias', 'h.9.crossattention.masked_bias', 'h.0.crossattention.q_attn.weight', 'h.1.crossattention.masked_bias', 'h.5.crossattention.q_attn.weight', 'h.7.ln_cross_attn.weight', 'h.4.ln_cross_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.9.crossattention.c_proj.weight', 'h.2.crossattention.c_attn.weight', 'h.5.ln_cross_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.ln_cross_attn.weight', 'h.10.crossattention.bias', 'h.0.crossattention.masked_bias', 'h.2.crossattention.masked_bias', 'h.10.crossattention.c_proj.weight', 'h.7.crossattention.c_proj.weight', 'h.3.crossattention.masked_bias', 'h.9.crossattention.bias', 'h.10.crossattention.c_proj.bias', 'h.0.crossattention.bias', 'h.5.crossattention.masked_bias', 'h.5.crossattention.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor\n",
    "\n",
    "image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
    "text_decode_model = \"gpt2\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    image_encoder_model, text_decode_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0fa24d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(image_encoder_model)\n",
    "# text tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_decode_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c61ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# update the model config\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640e6fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vit-gpt-model\\\\tokenizer_config.json',\n",
       " 'vit-gpt-model\\\\special_tokens_map.json',\n",
       " 'vit-gpt-model\\\\vocab.json',\n",
       " 'vit-gpt-model\\\\merges.txt',\n",
       " 'vit-gpt-model\\\\added_tokens.json',\n",
       " 'vit-gpt-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"vit-gpt-model\"\n",
    "model.save_pretrained(output_dir)\n",
    "feature_extractor.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b3263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://images.cocodataset.org/zips/train2017.zip\n",
    "#!wget http://images.cocodataset.org/zips/val2017.zip\n",
    "#!wget http://images.cocodataset.org/zips/test2017.zip\n",
    "#!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "#!wget http://images.cocodataset.org/annotations/image_info_test2017.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ab23b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://images.cocodataset.org/zips/train2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58ca75a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49eadc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m wget http://images.cocodataset.org/zips/train2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c7690b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m wget http://images.cocodataset.org/zips/val2017.zip\n",
    "#!python -m wget http://images.cocodataset.org/zips/test2017.zip\n",
    "#!python -m wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "#!python -m wget http://images.cocodataset.org/annotations/image_info_test2017.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23c3d7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset coco_dataset_script (C:/Users/Mayur/.cache/huggingface/datasets/coco_dataset_script/2017-66a97e8058d97425/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d2896e7afd488ea64507d6b62a3cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "COCO_DIR = \"C:/Users/Mayur/Desktop/project\"\n",
    "ds = datasets.load_dataset(\"C:/Users/Mayur/Desktop/project/ydshieh/coco_dataset_script.py\", \"2017\", data_dir=COCO_DIR)\n",
    "#mapping of images and captions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c138e915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset coco_dataset_script (C:/Users/Mayur/.cache/huggingface/datasets/coco_dataset_script/2017-data_dir=.%2Fdummy_data%2F/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d03c03019a491488d9181a8d3fc0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
       "        num_rows: 16\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "ds = datasets.load_dataset(\"C:/Users/Mayur/Desktop/project/ydshieh/coco_dataset_script.py\", \"2017\", data_dir=\"./dummy_data/\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "570788de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 74,\n",
       " 'caption_id': 145996,\n",
       " 'caption': 'A picture of a dog laying on the ground.',\n",
       " 'height': 426,\n",
       " 'width': 640,\n",
       " 'file_name': '000000000074.jpg',\n",
       " 'coco_url': 'http://images.cocodataset.org/train2017/000000000074.jpg',\n",
       " 'image_path': 'C:\\\\Users\\\\Mayur\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\4377c3242b1f37163494c9f4e56fa625634322fa7e9771ecd9c7e5404cdf40ca\\\\train2017\\\\000000000074.jpg'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print single example\n",
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f73fc2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# text preprocessing step\n",
    "def tokenization_fn(captions, max_target_length):\n",
    "    \"\"\"Run tokenization on captions.\"\"\"\n",
    "    labels = tokenizer(captions, \n",
    "                      padding=\"max_length\", \n",
    "                      max_length=max_target_length).input_ids\n",
    "\n",
    "    return labels\n",
    "\n",
    "# image preprocessing step\n",
    "def feature_extraction_fn(image_paths, check_image=True):\n",
    "    \"\"\"\n",
    "    Run feature extraction on images\n",
    "    If `check_image` is `True`, the examples that fails during `Image.open()` will be caught and discarded.\n",
    "    Otherwise, an exception will be thrown.\n",
    "    \"\"\"\n",
    "\n",
    "    model_inputs = {}\n",
    "\n",
    "    if check_image:\n",
    "        images = []\n",
    "        to_keep = []\n",
    "        for image_file in image_paths:\n",
    "            try:\n",
    "                img = Image.open(image_file)\n",
    "                images.append(img)\n",
    "                to_keep.append(True)\n",
    "            except Exception:\n",
    "                to_keep.append(False)\n",
    "    else:\n",
    "        images = [Image.open(image_file) for image_file in image_paths]\n",
    "\n",
    "    encoder_inputs = feature_extractor(images=images, return_tensors=\"np\")\n",
    "\n",
    "    return encoder_inputs.pixel_values\n",
    "\n",
    "def preprocess_fn(examples, max_target_length, check_image = True):\n",
    "    \"\"\"Run tokenization + image feature extraction\"\"\"\n",
    "    image_paths = examples['image_path']\n",
    "    captions = examples['caption']    \n",
    "    \n",
    "    model_inputs = {}\n",
    "    # This contains image path column\n",
    "    model_inputs['labels'] = tokenization_fn(captions, max_target_length)\n",
    "    model_inputs['pixel_values'] = feature_extraction_fn(image_paths, check_image=check_image)\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "656cc4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Mayur\\.cache\\huggingface\\datasets\\coco_dataset_script\\2017-data_dir=.%2Fdummy_data%2F\\0.0.0\\e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f\\cache-596bacd5ac28a506.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Mayur\\.cache\\huggingface\\datasets\\coco_dataset_script\\2017-data_dir=.%2Fdummy_data%2F\\0.0.0\\e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f\\cache-9dbd183764c0aa02.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Mayur\\.cache\\huggingface\\datasets\\coco_dataset_script\\2017-data_dir=.%2Fdummy_data%2F\\0.0.0\\e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f\\cache-e0a22c65ffa2c9c4.arrow\n"
     ]
    }
   ],
   "source": [
    "processed_dataset = ds.map(\n",
    "    function=preprocess_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"max_target_length\": 128},\n",
    "    remove_columns=ds['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9d68647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'pixel_values'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'pixel_values'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'pixel_values'],\n",
       "        num_rows: 16\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc912a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    output_dir=\"./image-captioning-output\",\n",
    "    num_train_epochs=10, # modify the number of epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aec1aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6306a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ignore_pad_token_for_loss = True\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if ignore_pad_token_for_loss:\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "                                                     decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds,\n",
    "                            references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad425c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=processed_dataset['train'],\n",
    "    eval_dataset=processed_dataset['validation'],\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0041530f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cfea8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./image-captioning-output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff64d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./image-captioning-output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6272181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 80\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 200\n",
      "  Number of trainable parameters = 239195904\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 1:41:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.336270</td>\n",
       "      <td>19.822200</td>\n",
       "      <td>1.922800</td>\n",
       "      <td>19.709000</td>\n",
       "      <td>19.665300</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.331767</td>\n",
       "      <td>20.044300</td>\n",
       "      <td>4.590100</td>\n",
       "      <td>18.363000</td>\n",
       "      <td>18.418900</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.345646</td>\n",
       "      <td>21.341000</td>\n",
       "      <td>3.222900</td>\n",
       "      <td>20.936900</td>\n",
       "      <td>21.019400</td>\n",
       "      <td>9.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.356260</td>\n",
       "      <td>23.855800</td>\n",
       "      <td>4.848000</td>\n",
       "      <td>22.281300</td>\n",
       "      <td>22.297200</td>\n",
       "      <td>10.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.374624</td>\n",
       "      <td>18.887800</td>\n",
       "      <td>1.665200</td>\n",
       "      <td>17.820300</td>\n",
       "      <td>17.884700</td>\n",
       "      <td>11.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.394013</td>\n",
       "      <td>19.189400</td>\n",
       "      <td>1.200700</td>\n",
       "      <td>18.102900</td>\n",
       "      <td>18.105800</td>\n",
       "      <td>11.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.402476</td>\n",
       "      <td>20.249500</td>\n",
       "      <td>2.585800</td>\n",
       "      <td>19.620500</td>\n",
       "      <td>19.674600</td>\n",
       "      <td>10.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412996</td>\n",
       "      <td>19.843200</td>\n",
       "      <td>1.202400</td>\n",
       "      <td>18.613900</td>\n",
       "      <td>18.645900</td>\n",
       "      <td>12.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.423060</td>\n",
       "      <td>15.183900</td>\n",
       "      <td>0.651300</td>\n",
       "      <td>14.477400</td>\n",
       "      <td>14.465700</td>\n",
       "      <td>12.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.425643</td>\n",
       "      <td>15.821900</td>\n",
       "      <td>0.984000</td>\n",
       "      <td>14.844300</td>\n",
       "      <td>14.840100</td>\n",
       "      <td>12.312500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 80\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 80\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 80\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 80\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 80\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 80\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 80\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 80\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 80\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 80\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.2680565643310547, metrics={'train_runtime': 6098.9629, 'train_samples_per_second': 0.131, 'train_steps_per_second': 0.033, 'total_flos': 1.443711128961024e+17, 'train_loss': 0.2680565643310547, 'epoch': 10.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73614a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd2771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f200459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d5e295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb5c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d66b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
